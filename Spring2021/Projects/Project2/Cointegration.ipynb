{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Unit Roots, Cointegration, and Error-Correction Models (Julia)__\n",
    "\n",
    "<br>\n",
    "\n",
    "Finance 5330: Financial Econometrics <br>\n",
    "Tyler J. Brough <br>\n",
    "Last Update: April 15, 2021 <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames\n",
    "using GLM\n",
    "using HypothesisTests\n",
    "using PyPlot; const plt = PyPlot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Unit Roots and Stationarity__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Random Walk with Drift\n",
    "\n",
    "<br>\n",
    "\n",
    "A simple starting model for efficient log-prices of assets is the ___Random Walk with Drift___ model:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "y_{t} = \\mu + y_{t-1} + \\epsilon_{t}, \\quad \\quad \\epsilon \\sim N(0, \\sigma_{\\epsilon}^{2})\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "with something like $y_{t} = \\ln{(p_{t})}$ with $p_{t}$ a transaction price observed in some market. The expected value of this process is: \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "E(y_{t}) = \\mu + y_{t-1}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "To get the variance it is helpful to solve recursively as follows, assuming $y_{0} = 0$ for simplicity:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "y_{t} = t \\mu + \\sum\\limits_{i=0}^{t} \\epsilon_{t-i}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "We can now state the variance of the process as:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "Var(y_{t}) = \\sum\\limits_{i=0}^{t} Var(\\epsilon_{t-i}) = Var(\\epsilon_{t}) + Var(\\epsilon_{t-1}) + \\cdots + Var(\\epsilon_{0}) = t \\sigma_{\\epsilon}^{2}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "From this it is easy to see that this is an explosive process as the variance is proportional to time. This translates to the value of the process at any time $t$ being unpredictable based on the information known up to that time ($I_{t-1}$). This is a good starting place for a model of informationally efficient prices as the definition of such is one that incorporates all available information up to that point in time. \n",
    "\n",
    "<br>\n",
    "\n",
    "__NB:__ Samuelson's paper: [Proof That Properly Anticipated Prices Fluctuate Randomly](https://www.gyc.com.sg/files/p_samuelson-proof.pdf)\n",
    "\n",
    "<br>\n",
    "\n",
    "We can simulate this process as follows (setting $\\mu = 0$ for convenience):\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cumsum(randn(52*5))\n",
    "plt.plot(y, color=\"orange\", linewidth=1.25)\n",
    "plt.grid(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Weak Stationarity__\n",
    "\n",
    "<br>\n",
    "\n",
    "In the time series literature a process is known as [weakly stationary](https://en.wikipedia.org/wiki/Stationary_process#Weak_or_wide-sense_stationarity) if the mean and autocovariance are not time varying. \n",
    "\n",
    "<br>\n",
    "\n",
    "A few notes:\n",
    "\n",
    "* Clearly the random walk process is __NOT__ weakly stationary.\n",
    "* A weakly stationary time series process will exhibit mean reversion\n",
    "* Mean reversion in a time series can be such that there is some predictibility in the process\n",
    "\n",
    "<br>\n",
    "\n",
    "It makes sense that informationally efficient prices should behave as a random walk model (with possible other extensions).\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "The random walk model is a special case of the AR(1) model:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "y_{t} = \\phi y_{t-1} + \\epsilon_{t}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "It won't be shown here, but a technical requirement for the AR(1) model to be weakly stationary is $|\\phi| < 1$. For the random walk model $\\phi = 1$, thus the alternative name ___unit root___.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### __Some Notation__\n",
    "\n",
    "<br>\n",
    "\n",
    "A random walk model is also known as a unit-root non-stationary process. In the literature this is often denoted as $y_{t} \\sim I(1)$. We state this as: \"_the process $y_{t}$ is_ ___integrated of order one___.\"\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "We can tranform and $I(1)$ process to a stationary process by ___first differencing___ the process like so:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{t} - y_{t-1} &= y_{t-1} - y_{t-1} + \\epsilon_{t} \\\\\n",
    "y_{t} - y_{t-1} &= \\epsilon_{t} \\\\\n",
    "\\Delta y_{t}    &= \\epsilon_{t}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "In this case we can denote that $\\Delta y_{t}$ is now weakly stationary with the notation $\\Delta y_{t} \\sim I(0)$, i.e. \"$\\Delta y_{t}$ _is_ ___integrated of order zero.___\"\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Spurious Regression__\n",
    "\n",
    "<br>\n",
    "\n",
    "It is important to understand the properties of unit root processes, because they can be problematic to work with in applying econometrics to finance. \n",
    "\n",
    "<br>\n",
    "\n",
    "For example, there is a well-known problem of ___spurious regression___ when one unit root process is regressed on an independent unit root process:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "y_{t} = \\alpha + \\beta x_{t} + u_{t}, \\quad u_{t} \\sim N(0, \\sigma_{u}^{2})\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "This regression is not valid because the homoscedasticity assumption of the error term is violated (recall that $Var(y_{t}) = t\\sigma_{\\epsilon}^{2}$)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "This can be easily demonstrated by a simple Monte Carlo study as follows:\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10_000\n",
    "N = 52 * 5\n",
    "β̂ = zeros(M)\n",
    "R² = zeros(M);\n",
    "tstats = zeros(M);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Monte Carlo simulation loop\n",
    "for i in 1:M\n",
    "    y = cumsum(randn(N))\n",
    "    x = cumsum(randn(N))\n",
    "    data = DataFrame(Y=y, X=x)\n",
    "    reg = GLM.lm(@formula(Y ~ X), data);\n",
    "    β̂[i] = coef(reg)[2]\n",
    "    R²[i] = r2(reg)\n",
    "    tstats[i] = coef(reg)[2] / stderror(reg)[2]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __The Histogram $\\hat{\\beta}$__\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "Let's first take a look at the histogram just for the $\\hat{\\beta}$'s. \n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(β̂, bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The distribution is centered at zero, but it has pretty thick tails! Way thicker than they should be. We should basically see something that looks like a point distribution at zero!\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __The Histogram for the $T$-Statistic__\n",
    "\n",
    "<br>\n",
    "\n",
    "For each repitition of the simulation we test the following hypothesis test:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H_{0}: & \\beta = 0   \\\\\n",
    "       &             \\\\\n",
    "H_{a}: & \\beta \\ne 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "where the test statistic is the $t$-statistic:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "t = \\frac{\\hat{\\beta}}{se(\\hat{\\beta})}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Recall that the critical value is about $2$-ish for a $99\\%$ confidence level. So one quick check will be to see how many of the simulations incorrectly reject the null hypothesis. \n",
    "\n",
    "<br>\n",
    "\n",
    "__NB:__ recall that we need to look at the absolute value of the $t$-statistics to account for the two-tailed nature of the test.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion = sum((abs.(tstats) .> 2.0)) / M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa! A ___whopping___ $85\\%$ of the repetitions falsley reject the null hypothesis. Clearly this is a problem.\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's see the histogram.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tstats, bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "__Those are some awefully thick tails!!!__\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __The Histogram for $R^{2}$__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Rsqrd, bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We can also see from histogram of the $R^{2}$ that there are some extremely high values even though we know that the processes are independent!\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLM.stderror(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstat = coef(reg)[2] / stderror(reg)[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## __Testing for Unit Roots__\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __The Dickey-Fuller Test for Unit Roots__\n",
    "\n",
    "<br>\n",
    "\n",
    "We can test for unit roots in a time series process with the so-called ___Dickey-Fuller Test___, named for the statisticians who invented it. \n",
    "\n",
    "<br>\n",
    "\n",
    "It would seem natural to test the following hypothesis: \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H_{0}: \\phi = 1 \\\\\n",
    "H_{a}: \\phi \\ne 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "for the regression: $y_{t} = \\phi y_{t-1} + \\epsilon_{t}$, but because the model under the null hypothesis leads to spurious regression we cannot conduct this direct test. \n",
    "\n",
    "<br>\n",
    "\n",
    "D-F had the bright idea to transform the model to render it amenable to such testing. We start by subtracting $y_{t-1}$ from both sides:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{t} - y_{t-1} &= \\phi y_{t-1} - y_{t-1} + \\epsilon_{t} \\\\\n",
    "\\Delta y_{t}    &= (\\phi - 1) y_{t-1} + \\epsilon_{t} \\\\\n",
    "\\Delta y_{t}    &= \\theta y_{t-1} + \\epsilon_{t}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "where $\\theta = \\phi - 1$. With this transformation we can now conduct the test:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H_{0}: \\theta = 0 \\\\\n",
    "H_{a}: \\theta \\ne 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Because $\\Delta y_{t} \\sim I(0)$ and when $\\phi = 1$ it means that $\\theta = 0$ this model is now valid under the null hypothesis. We can form the standard $t$-ratio as our test statistic, but D-F showed that the asymptotic sampling distribution of $t$ is no longer the Standard Normal distribution. Instead they provide critical values via a Monte Carlo method. \n",
    "\n",
    "<br>\n",
    "\n",
    "### __The Augmented Dickey-Fuller Test__\n",
    "\n",
    "<br>\n",
    "\n",
    "D-F added one extension to the test to account for possible serial correlation in $\\Delta y_{t}$. The model under the null hypothesis now becomes:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\Delta y_{t} = \\theta y_{t-1} + \\sum\\limits_{i=1}^{p} \\gamma_{i} \\Delta y_{t-i} + \\epsilon_{t}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "This tends to make the test more robust to short-term serial correlations in the process.\n",
    "\n",
    "<br>\n",
    "\n",
    "We can use the `HypothesisTests` package to conduct the ADF test as follows. See [ADFTest](https://juliastats.org/HypothesisTests.jl/stable/time_series/#Augmented-Dickey-Fuller-test-1) for more details.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?ADFTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Simple `ADFTest` Example\n",
    "\n",
    "Let's do a simple test. We will simulate a pure random walk process. The ADF test should fail to reject the null hypothesis.\n",
    "\n",
    "<br>\n",
    "\n",
    "__NB:__ Recall that the null hypothesis is $H_{0}: \\theta = 0$, where $\\theta = (\\phi - 1.0)$ which will be zero when $\\phi = 1.0$ (which is the definition of a unit root)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cumsum(randn(52 * 5))\n",
    "test = ADFTest(y, :none, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Not surprisingly, we __fail to reject__!\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can get the $p$-value with the `pvalue` method\n",
    "pvalue(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### __Recapitulation__\n",
    "\n",
    "With the ADF test we now have a reliable way to detect the presence of unit-root non-stationarity. \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Cointegration and Error-Correction Model Forms__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Cointegration__\n",
    "\n",
    "<br>\n",
    "\n",
    "Fortunately, there is an upside to unit-root non-stationarity for financial modeling. It turns out that there is a relationship that is even stronger when pairs of asset prices are $I(1)$, but move together in a way. This concept is ___cointegration___. \n",
    "\n",
    "<br>\n",
    "\n",
    "If there is a linear combination of two processes that are separately $I(1)$ that is itself $I(0)$, we say that the two processes are ___cointegrated___. Cointegration is a stronger concept than mere correlation. It has a causal explanation. I often like to say that (at least in financial applications) ___cointegration is the statistical footprint of an arbitrage relationship.___\n",
    "\n",
    "<br>\n",
    "\n",
    "We can test for cointegration using the ADF test developed above. To begin, set up and run the following regression:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "y_{t} = \\alpha + \\beta x_{t} + u_{t}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "If the variables are cointegrated this is not a spurious regression. In fact, it has the property of superconsistency (a kind of uber statistical efficiency). \n",
    "\n",
    "<br>\n",
    "\n",
    "Once the model is estimated, we can form the fitted residuals:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{u_{t}} &= y_{t} - \\hat{\\alpha} - \\hat{\\beta} x_{t} \\\\\n",
    "\\hat{u_t{}} &= y_{t} - \\hat{y_{t}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "We can now submit these fitted residuals to the ADF test as above. \n",
    "\n",
    "<br>\n",
    "\n",
    "__NB:__ the null hypothesis of the ADF test is that there ___is___ a unit-root. Cointegration exists between $y_{t}$ and $x_{t}$ if there ___is not___ a unit-root in $\\hat{u_{t}}$. So we conclude that there is cointegration if we reject the null hypothesis of the ADF test. \n",
    "\n",
    "<br>\n",
    "\n",
    "We can simulate this as follows.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 52 * 5\n",
    "x = cumsum(randn(N))\n",
    "u = randn(N)\n",
    "y = 0.22 .+ 0.75 .* x .+ u;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y, color=\"orange\", linewidth=1.0)\n",
    "plt.plot(x, color=\"purple\", linewidth=1.0)\n",
    "plt.title(\"Simulated Cointegration\")\n",
    "plt.grid(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataFrame(Y=y, X=x)\n",
    "reg = GLM.lm(@formula(Y ~ X), data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ϵ̂ = residuals(reg)\n",
    "plt.plot(ϵ̂, color=\"blue\", linewidth=1.0)\n",
    "plt.grid(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Conduct the ADFTest\n",
    "test = ADFTest(ϵ̂, :none, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "__NB: We strongly reject the null hypothes! (meaning that we accept that these variables are cointegrated)__\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalue(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Error-Correction Models__\n",
    "\n",
    "<br>\n",
    "\n",
    "Whenever two (or more) asset prices are cointegrated, we can also write down an error-correction model. That is, cointegration implies an error-correction form. \n",
    "\n",
    "<br>\n",
    "\n",
    "We state the simplest form of the error-correction model as follows:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Delta y_{t} &= \\lambda (y_{t-1} - \\alpha - \\beta x_{t-1}) + \\nu_{t} \\\\\n",
    "             &  \\\\\n",
    "\\Delta y_{t} &= \\lambda (z_{t-1}) + \\nu_{t}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "with $z_{t} = \\hat{u}_{t} = (y_{t-1} - \\alpha - \\beta x_{t-1})$.\n",
    "\n",
    "<br>\n",
    "\n",
    "This model form relates the changes in $y_{t}$, that is $\\Delta y_{t}$ to the ___spread___ between $y_{t-1}$ and $x_{t-1}$ in ___levels___. This is a valid time series regression, given that $\\Delta y_{t} \\sim I(0)$ via first differencing, and $z_{t-1} \\sim I(0)$ via cointegration. \n",
    "\n",
    "<br>\n",
    "\n",
    "Let's see if we can develop some intuition for this model. Let's start by interpreting the coefficient $\\boldsymbol{\\lambda}$, which we call the ___error-correction coefficient___. Its value will be such that when there is a large past deviation between $y_{t-1}$ and $x_{t-1}$ (i.e. a large error) it will cause an ___error correction___ in the change in $y_{t}$, or $\\Delta y_{t}$. In other words, $\\Delta y_{t}$ will adjust based on a lagged error in the spread. There is now a stationary relationship (i.e. mean-reverting) that exists, and can even be predicted. It's easy to see now why we call this an error-correction model, and also its relationship with cointegration.\n",
    "\n",
    "<br>\n",
    "\n",
    "* __Q:__ What causes the error-correction in $\\Delta y_{t}$? \n",
    "* __A:__ in financial markets between related asset prices that are cointegrated, the answer is ___arbitrage___!\n",
    "\n",
    "<br>\n",
    "\n",
    "We can now think about the error-correction model in terms of some kind of equilibrium concept. When dynamic market forces are such that related asset prices are temporarily driven apart, an arbitrage relationship between the asset prices acts to restore the spread between the two to a long-run equilibrium level. \n",
    "\n",
    "<br>\n",
    "\n",
    "* __Q:__ what kind of equilibrium concept fits this description? \n",
    "* __Q:__ is it a static neo-classical equilibrium? \n",
    "* __Q:__ is it more like the neo-Austrian type of equilibrium that has been mentioned in this class?\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### __A More General Error-Correction Model__\n",
    "\n",
    "<br>\n",
    "\n",
    "We can also account for possible short-run variation in $\\Delta y_{t}$ by adding lagged terms on the right-hand side of the model as follows (as well as a drift term):\n",
    "\n",
    "$$\n",
    "\\Delta y_{t} = \\mu + \\sum\\limits_{j=1}^{p} \\delta_{j} \\Delta y_{t-j} + \\lambda z_{t-1} + \\nu_{t}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "### __A Vector Error-Correction Model__\n",
    "\n",
    "Now we can think in terms of systems of equations, and think about a multivariate relationship between $y_{t}$ and $x_{t}$ called a ___vector error-correction model___ (vecm).\n",
    "\n",
    "<br>\n",
    "\n",
    "Here is a VECM(1) model in $y_{t}$ and $x_{t}$:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Delta y_{t} &= \\mu_{1} + \\delta_{1} \\Delta y_{t-1} + \\gamma_{1} \\Delta x_{t-1} + \\lambda_{1} z_{t-1} + \\nu_{1,t} \\\\\n",
    "& \\\\\n",
    "\\Delta x_{t} &= \\mu_{1} + \\delta_{2} \\Delta y_{t-1} + \\gamma_{2} \\Delta x_{t-1} + \\lambda_{2} z_{t-1} + \\nu_{2,t}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}