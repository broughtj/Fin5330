# __Brough Lecture Notes: Beginning Time Series Topics__

<br>

Finance 5330: Financial Econometrics <br>
Tyler J. Brough <br>
Last Updated: January 25, 2019 <br>
<br>

___Beginning Time Series Topics___

<br>

Most data in economics (espcially in macroeconomics and finance) come int he form of _time series_.

<br>

___Time Series:___ a set of repeated observations of the same random variable ordered in time. 

* Example: GNP or stock returns

* Also: prices, exchange rates, interest rates, inflation (lots of others)

<br>

We can write a time series as $\{x_{1}, x_{2}, \ldots, x_{T}\}$ or simply as $\{x_{t}\}_{t=1}^{T}$.

<br>

We treat $x_{t}$ as a random variable. Really nothing different from the rest of econometrics. Notice the difference is the subscript $t$ rather than $i$.

<br>

If, for example, a random variable $y_{t}$ is generated by

$$
y_{t} = x_{t} \beta + \varepsilon_{t}
$$

<br>

in which $E(y_{t}|x_{t}) = 0$

Then OLS provides a consistent estimate for $\beta$ (just as if the subscript were "$i$" instead of "$t$").

<br>

The phrase "time series" is used to denote

1. a sample $\{x_{t}\}$ such as IBM stock price from Jan. 1, 2010 to Dec. 31, 2010.

2. A probability model for that sample. i.e. a statement about the joint distribution of the random variables $\{x_{t}\}$.

A first model for the joint distribution of a time series $\{x_{t}\}$ is: 

$$
x_{t} = \varepsilon_{t}, \quad \varepsilon_{t} \sim N(0, \sigma_{\varepsilon}^{2})
$$

<br>

i.e. $x_{t}$ is normal and independent over time. 

<br>

Typically, time series are not iid, which is what makes them interesting. 

<br>

Ex: unusually high inflation today is likely to lead to unusually high inflation tomorrow. 


(1) and (2) are the absence of any serial correlation or predictability. 

<br>

(3) is conditional homoscedasticity or a constant conditional variance. 

<br>
<br>

By itself $\varepsilon_{t}$ is pretty boring. If $\varepsilon_{t}$ is abnormally high there is no tendency for $\varepsilon_{t+1}$ to be high. 

<br>

More realistic models are constructed by taking combinations of $\varepsilon_{t}$.

___Basic ARMA Models___


<br>

Most of the time our time series models will be created by taking linear combinations of white noise

* AR(1): $\quad \quad \quad x_{t} = \phi x_{t-1} + \varepsilon_{t}$

* MA(1): $\quad \quad \quad x_{t} = \varepsilon_{t} + \theta \varepsilon_{t-1}$

* AR(p): $\quad \quad \quad x_{t} = \phi_{1} x_{t-1} + \phi_{2} x_{t-2} + \cdots + \phi_{p} x_{t-p} + \varepsilon_{t}$

* MA(q): $\quad \quad \quad x_{t} = \varepsilon_{t} + \theta_{1} \varepsilon_{t-1} + \theta_{2} \varepsilon_{t-2} + \cdots + \theta_{q} \varepsilon_{t-q}$

* ARMA(p,q): $\quad x_{t} = \phi_{1} x_{t-1} + \cdots + \phi_{p} x_{t-p} + \varepsilon_{t} + \theta_{1} \varepsilon_{t-1} + \cdots + \theta_{q} \varepsilon_{t-q}$


Notice that each model is a recipe to generate a sequence $\{x_{t}\}$ given a sequence of realizations of the white noise process and a starting $x_{0}$ value.

<br>

All of these models are mean zero, and represent deviations of the series about a mean. For example, if a series has mean $\bar{x}$ and follows an AR(1)

$$
(x_{t} - \bar{x}) = \phi(x_{t-1} - \bar{x}) + \varepsilon_{t}
$$

is equivalent to 

$$
\begin{aligned}
x_{t} &= (1 - \phi)\bar{x} + \phi x_{t-1} + \varepsilon_{t} \\
      &= \mu + \phi x_{t-1} + \varepsilon_{t}
\end{aligned}
$$

<br>

where $\mu = (1 - \phi)\bar{x}$

<br>

__NB:__ the constant absorbs the mean


__Lag Operators and Polynomials___

<br>

It is easiest to represent ARMA models in _lag operator_ notation. The lag operator moves the index back one time unit:

$$
L^{1}x_{t} = x_{t-1}
$$

<br>
<br>

More formally, $L$ is an operator that takes an original time series $\{x_{t}\}$ and produces another, which is the same as the original only shifted backwards in time.

<br>

From the definition we can do other things:

$$
\begin{aligned}
L^{2} x_{t}  &= L(L x_{t}) = L x_{t-1} = x_{t-2} \\
L^{j} x_{t}  &= x_{t-j} \\
L^{-j} x_{t} &= x_{t+j} \\
\end{aligned}
$$

<br>

We can also define lag polynomials, e.g.

$$
a(L) = (a_{0} L + a_{1} L^{1} + a_{2} L^{2}) x_{t} = a_{0} x_{t} + a_{1} x_{t-1} + a_{2} x_{t-2}
$$

<br>

Using this notation we can rewrite the ARMA models as

* AR(1): $\quad \quad \quad (1 - \phi L) x_{t} = \varepsilon_{t}$

* MA(1): $\quad \quad \quad x_{t} = (1 + \theta L) \varepsilon_{t}$

* AR(p): $\quad \quad \quad (1 - \phi_{1} L - \phi_{2} L^{2} + \cdots + \phi_{p} L^{p}) x_{t} = \varepsilon_{t}$

* MA(q): $\quad \quad \quad x_{t} = (1 + \theta_{1} L + \theta_{2} L^{2} + \cdots + \theta_{q} L^{q}) \varepsilon_{t}$

<br>

ARMA models are not unique. A time series with a given joint distribution of $\{x_{0}, x_{1}, \ldots, x_{T}\}$ can usually be represented with a variety of ARMA models.

<br>

It is often convenient to work with different representations:

1. The shortest (or only finite length) polynomial representation is usually the easiest to work with

2. AR forms are the easiest to estimate (since OLS assumptions still apply)

3. MA forms express $x_{t}$ in terms of a linear combination of independent right hand side variables. Often finding variances and covariances in this form is easiest.

<br>

___AR(1) to MA($\infty$) by Recursive Substitution___


<br>

Start with an AR(1)

$$
x_{t} = \phi x_{t-1} + \varepsilon_{t}
$$

<br>

Recursively substituting

$$
\begin{aligned}
x_{t} &= \phi(\phi x_{t-2} + \varepsilon_{t-1}) + \varepsilon_{t} = \phi^{2} x_{t-2} + \phi \varepsilon_{t-1} + \varepsilon_{t} \\
x_{t} &= \phi^{k} x_{t-k} + \phi^{k-1} \varepsilon_{t-k+1} + \cdots + \phi^{2} \varepsilon_{t-2} + \phi \varepsilon_{t-1} + \varepsilon_{t} \\
\end{aligned}
$$

<br>

Thus an AR(1) can always be expressed as an ARMA(k,k-1).

Also, if $|\phi| < 1$ so that $$\lim_{k\to\infty} \phi^{k} x_{t-k} = 0$$ then

$$
x_{t} = \sum\limits_{j=0}^{\infty} \phi^{j} \varepsilon_{t-j}
$$

<br>
<br>
<br>

___AR(1) to MA($\infty$) with Lag Operators___

<br>

Starting again with the AR(1) model:

$$
(1 - \phi L) x_{t} = \varepsilon_{t}
$$

<br>

The way to "invert"  the AR(1) is to write

$$
x_{t} = (1 - \phi L)^{-1} \varepsilon_{t}
$$

hat does $(1 - \phi L)^{-1}$ mean? We have only defined polynomials in $L$ so far.

<br>

We try to use the expression

$$
(1 - z)^{-1} = 1 + z + z^{2} + z^{3} + \cdots \quad \mbox{for} \quad |z| < 1
$$

__NB:__ this expression for $z$ can be proven with a Taylor expansion.

<br>

Using this expansion and hoping that $|\phi| < 1$ implies $|\phi L| < 1$, suggests

<br>

$$
x_{t} = (1 - \phi L)^{-1} \varepsilon_{t} = (1 + \phi L + \phi^{2} L^{2}  + \cdots) \varepsilon_{t} = \sum\limits_{j=0}^{\infty} \phi^{j} \varepsilon_{t-j}
$$

<br>

Note: we can't always perform this inversion. We require $|\phi| < 1$. Not all ARMA processes are invertible to a representation of $x_{t}$ in terms of current and past $\varepsilon_{t}$

<br>
<br>

___AR(p) to MA($\infty$)___

<br>

Getting to an MA($\infty$) from an AR(1) is almost as easy either way (recursive substitution or lag operators) but in higher order models lag operators become much easier. 

<br>

Let's try an AR(2):

$$
\begin{aligned}
x_{t} &= \phi_{1} x_{t-1} + \phi_{2} x_{t-2}  + \varepsilon_{t} \\
(1 - \phi_{1} L - \phi_{2} L^{2}) x_{t} &= \varepsilon_{t}
\end{aligned}
$$

<br>

We need to factor $(1 - \phi_{1} L - \phi_{2} L^{2})$ in order to use the $(1 - z)^{-1}$ formula. So find $\lambda_{1}$ and $\lambda_{2}$ such that

<br>

$$
(1 - \phi_{1} L - \phi_{2} L^{2}) = (1 - \lambda_{1} L) (1 - \lambda_{2} L)
$$

<br>

The solution is 

$$
\begin{aligned}
\lambda_{1} \lambda_{2}   &= -\phi_{2} \\
\lambda_{1} + \lambda_{2} &= \phi_{1} 
\end{aligned}
$$

<br>
<br>

$$
\begin{aligned}
(1 - \lambda_{1} L) (1 - \lambda_{2} L) &= 1 - \lambda_{2} L - \lambda_{1} L + \lambda_{1} \lambda_{2} L \\
                                        &= 1 - (\lambda_{1} + \lambda_{2})L + \lambda_{1} \lambda_{2} L
\end{aligned}
$$

<br>

Now we need to invert

$$
(1 - \lambda_{1} L) (1 - \lambda_{2} L) x_{t} = \varepsilon_{t}
$$

<br>

Thus

$$
\begin{aligned}
x_{t} &= (1 - \lambda_{1} L)^{-1} (1 - \lambda_{2})^{-1} x_{t} = \varepsilon_{t} \\
x_{t} &= \left[\sum\limits_{j=0}^{\infty} \lambda_{1}^{j} L^{j}\right] \left[\sum\limits_{j=0}^{\infty} \lambda_{2}^{j} L^{j} \right] \varepsilon_{t}
\end{aligned}
$$

<br>

Multiplying out the polynomials is tedious but straight forward

<br>

$$
\begin{aligned}
\left[\sum\limits_{j=0}^{\infty} \lambda_{1}^{j} L^{j}\right] \left[\sum\limits_{j=0}^{\infty} \lambda_{2}^{j} L^{j} \right] &= (1 + \lambda_{1} L + \lambda_{2} L^{2} + \cdots) (1 + \lambda_{2} L + \lambda_{2} L^{2} + \cdots) \\
&= 1 + (\lambda_{1} + \lambda_{2}) L + (\lambda_{1}^{2} + \lambda_{1} \lambda_{2} + \lambda_{2}^{2}) L^{2} + \cdots \\
&= \sum\limits_{j=0}^{\infty} \left(\sum\limits_{k=0}^{j} \lambda_{1}^{k} \lambda_{2}^{j-k}\right) L^{j}
\end{aligned}
$$

<br>
<br>

A nicer way to express an MA($\infty$) is to use the __partial fractions trick__. Find $a$ and $b$ such that

<br>

$$
\begin{aligned}
\frac{1}{(1 - \lambda_{1} L) (1 - \lambda_{2} L)} &= \frac{a}{(1 - \lambda_{1} L)} + \frac{b}{(1 - \lambda_{2} L)} \\
&= \frac{a(1 - \lambda_{2} L) + b(1 - \lambda_{1} L)}{(1 - \lambda_{1} L)(1 - \lambda_{2} L)}
\end{aligned}
$$

<br>

The right-hand side numerator must equal $1$, so

$$
\begin{aligned}
a + b &= 1 \\
a \lambda_{2} + b \lambda_{1} & = 0
\end{aligned}
$$

<br>

The solution is

$$
b = \frac{\lambda_{2}}{\lambda_{2} - \lambda_{1}}, \quad a = \frac{\lambda_{1}}{\lambda_{1} - \lambda_{2}}
$$

<br>

$$
\frac{1}{(1 - \lambda_{1} L) (1 - \lambda_{2} L)} = \frac{\lambda_{1}}{(\lambda_{1} - \lambda_{2})} \frac{1}{(1 - \lambda_{1} L)} + \frac{\lambda_{2}}{(\lambda_{2} - \lambda_{1})} \frac{1}{(1 - \lambda_{2} L)}
$$

<br>

Thus we can express $x_{t}$ as

<br>

$$
\begin{aligned}
x_{t} &= \frac{\lambda_{1}}{\lambda_{1} - \lambda_{2}} \sum\limits_{j=0}^{\infty} \lambda_{1}^{j} \varepsilon_{t-j} + \frac{\lambda_{2}}{\lambda_{2} - \lambda_{1}} \sum\limits_{j=0}^{\infty} \lambda_{2}^{j} \varepsilon_{t-j} \\
&= \sum\limits_{j=0}^{\infty} \left( \frac{\lambda_{1}}{\lambda_{1} - \lambda_{2}}  \lambda_{1}^{j} + \frac{\lambda_{2}}{\lambda_{2} - \lambda_{1}} \lambda_{2}^{j} \right) \varepsilon_{t-j}
\end{aligned}
$$

<br>
<br>

__Note:__ Again not every AR(2) can be inverted. We require that the $\lambda$'s satisfy $|\lambda| < 1$.

<br>
<br>

Until explicitly stated we will assume we are working with invertible ARMA models.

<br>
<br>

___MA(q) to AR($\infty$)___

<br>

This is now straight forward

$$
x_{t} = b(L) \varepsilon_{t}
$$

<br>

has AR($\infty$) representation

<br>

$$
b(L)^{-1} x_{t} = \varepsilon_{t}
$$