---
title: Finance 5330 - Financial Econometrics
subtitle: Time Series Notes I
author: Tyler J. Brough
institute: Department of Finance and Economics 
titlegraphic: ./images/vertical-logo-blue.png
fontsize: 10pt
output:
 beamer_presentation:
    template: ./beamer-template.tex
    keep_tex: false
    toc: true
#    slide_level: 2
 ioslides_presentation:
    smaller: true
#    logo: ~/Dropbox/teaching/clemson-paw-transparent.png
make149: true
---

<style>
slides > slide.backdrop {
  background: white;
  border-bottom: 0px;
  box-shadow: 0 0 0;
}


slides > slide {
  font-family: 'Open Sans', Helvetica, Arial, sans-serif;
  border-bottom: 3px solid  #F66733;
  box-shadow:  0 3px 0 #00488D;

}

.title-slide hgroup h1 {
  color: #00488D;
}



h2 {

  color: #00488D;
}

slides > slide.dark {
  background: #00488D !important;
  border-bottom: 0;
  box-shadow: 0 0 0;
}

.segue h2 {
  color: white;
}

slides > slide.title-slide {
  border-bottom: 0;
  box-shadow: 0 0 0;
}

ol, ul {

padding-bottom: 10px;

}

</style>


# Introduction to Time Series I

## __Beginning Time Series Topics__


Most data in economics (espcially in macroeconomics and finance) come in the form of _time series_.

\vspace{3mm}

__Time Series:__ a set of repeated observations of the same random variable ordered in time. 

\vspace{3mm}

* Example: GNP or stock returns

* Also: prices, exchange rates, interest rates, inflation (lots of others)


##

We can write a time series as $\{x_{1}, x_{2}, \ldots, x_{T}\}$ or simply as $\{x_{t}\}_{t=1}^{T}$.

\vspace{3mm}

We treat $x_{t}$ as a random variable. Really nothing different from the rest of econometrics. Notice the difference is the subscript $t$ rather than $i$.

\vspace{3mm}

If, for example, a random variable $y_{t}$ is generated by

$$
y_{t} = x_{t} \beta + \varepsilon_{t}
$$

\vspace{3mm}

in which $E(y_{t}|x_{t}) = 0$


##

Then OLS provides a consistent estimate for $\beta$ (just as if the subscript were "$i$" instead of "$t$").

\vspace{3mm}

The phrase "time series" is used to denote:

\vspace{2mm}

1. a sample $\{x_{t}\}$ such as IBM stock price from Jan. 1, 2010 to Dec. 31, 2010.

2. A probability model for that sample. i.e. a statement about the joint distribution of the random variables $\{x_{t}\}$.


## 

A first model for the joint distribution of a time series $\{x_{t}\}$ is: 

$$
x_{t} = \varepsilon_{t}, \quad \varepsilon_{t} \sim N(0, \sigma_{\varepsilon}^{2})
$$

\vspace{2.5mm}

i.e. $x_{t}$ is normal and independent over time. 

\vspace{3mm}

Typically, time series are not iid, which is what makes them interesting. 

\vspace{3mm}

Ex: unusually high inflation today is likely to lead to unusually high inflation tomorrow. 


##

The building block for our time series models is the ___white noise process___

\vspace{3mm}

$$
\varepsilon_{t} \sim \mbox{ iid }  N(0, \sigma_{\varepsilon}^{2})
$$
\vspace{3mm}

Note three implications: 

\vspace{2.5mm}

1. $E(\varepsilon_{t}) = E(\varepsilon_{t} | \varepsilon_{t-1}, \varepsilon_{t-2}, \ldots) = E(\varepsilon_{t} | \mbox{ all info at t-1}) = 0$

2. $E(\varepsilon_{t} \varepsilon_{t-j}) = Cov(\varepsilon_{t} \varepsilon_{t-j}) = 0$

3. $Var(\varepsilon_{t}) = Var(\varepsilon_{t} | \varepsilon_{t-1}, \varepsilon_{t-2}, \ldots) = Var(\varepsilon_{t} | \mbox{ all info at t-1}) = \sigma_{\varepsilon_{t}^{2}}$


## 

$(1)$ and $(2)$ are the absence of any serial correlation or predictability. 

\vspace{3mm}

$(3)$ is conditional homoscedasticity or a constant conditional variance. 

\vspace{5mm}

By itself $\varepsilon_{t}$ is pretty boring. If $\varepsilon_{t}$ is abnormally high there is no tendency for $\varepsilon_{t+1}$ to be high. 

\vspace{2mm}

More realistic models are constructed by taking combinations of $\varepsilon_{t}$.


## Basic ARMA Models


Most of the time our time series models will be created by taking linear combinations of white noise

\vspace{3mm}

* AR(1): $\quad \quad \quad x_{t} = \phi x_{t-1} + \varepsilon_{t}$

* MA(1): $\quad \quad \quad x_{t} = \varepsilon_{t} + \theta \varepsilon_{t-1}$

* AR(p): $\quad \quad \quad x_{t} = \phi_{1} x_{t-1} + \phi_{2} x_{t-2} + \cdots + \phi_{p} x_{t-p} + \varepsilon_{t}$

* MA(q): $\quad \quad \quad x_{t} = \varepsilon_{t} + \theta_{1} \varepsilon_{t-1} + \theta_{2} \varepsilon_{t-2} + \cdots + \theta_{q} \varepsilon_{t-q}$

* ARMA(p,q): $\quad x_{t} = \phi_{1} x_{t-1} + \cdots + \phi_{p} x_{t-p} + \varepsilon_{t} + \theta_{1} \varepsilon_{t-1} + \cdots + \theta_{q} \varepsilon_{t-q}$


##

Notice that each model is a recipe to generate a sequence $\{x_{t}\}$ given a sequence of realizations of the white noise process and a starting $x_{0}$ value.

\vspace{3mm}

All of these models are mean zero, and represent deviations of the series about a mean. For example, if a series has mean $\bar{x}$ and follows an AR(1)

$$
(x_{t} - \bar{x}) = \phi(x_{t-1} - \bar{x}) + \varepsilon_{t}
$$ 

is equivalent to 

$$
\begin{aligned}
x_{t} &= (1 - \phi)\bar{x} + \phi x_{t-1} + \varepsilon_{t} \\
      &= \mu + \phi x_{t-1} + \varepsilon_{t}
\end{aligned}
$$
\vspace{2mm}

where $\mu = (1 - \phi)\bar{x}$

\vspace{2mm}

__NB:__ the constant absorbs the mean


## Lag Operators and Polynomials

\vspace{3mm}

It is easiest to represent ARMA models in _lag operator_ notation. The lag operator moves the index back one time unit:

$$
Lx_{t} = x_{t-1}
$$

\vspace{4mm}

More formally, $L$ is an operator that takes an original time series $\{x_{t}\}$ and produces another, which is the same as the original only shifted backwards in time.


##

From the definition we can do other things:

$$
\begin{aligned}
L^{2} x_{t}  &= L(L x_{t}) = L x_{t-1} = x_{t-2} \\
L^{j} x_{t}  &= x_{t-j} \\
L^{-j} x_{t} &= x_{t+j} \\
\end{aligned}
$$

\vspace{3mm}

We can also define lag polynomials, e.g.

$$
a(L) = (a_{0} L + a_{1} L^{1} + a_{2} L^{2}) x_{t} = a_{0} x_{t} + a_{1} x_{t-1} + a_{2} x_{t-2}
$$


##

Using this notation we can rewrite the ARMA models as

\vspace{3mm}

* AR(1): $\quad \quad \quad (1 - \phi L) x_{t} = \varepsilon_{t}$

* MA(1): $\quad \quad \quad x_{t} = (1 + \theta L) \varepsilon_{t}$

* AR(p): $\quad \quad \quad (1 - \phi_{1} L - \phi_{2} L^{2} + \cdots + \phi_{p} L^{p}) x_{t} = \varepsilon_{t}$

* MA(q): $\quad \quad \quad x_{t} = (1 + \theta_{1} L + \theta_{2} L^{2} + \cdots + \theta_{q} L^{q}) \varepsilon_{t}$


##

ARMA models are not unique. A time series with a given joint distribution of $\{x_{0}, x_{1}, \ldots, x_{T}\}$ can usually be represented with a variety of ARMA models.

\vspace{3mm}

It is often convenient to work with different representations:

\vspace{2mm}

1. The shortest (or only finite length) polynomial representation is usually the easiest to work with

2. AR forms are the easiest to estimate (since OLS assumptions still apply)

3. MA forms express $x_{t}$ in terms of a linear combination of independent right hand side variables. Often finding variances and covariances in this form is easiest.


## AR(1) to MA($\infty$) by Recursive Substitution

\vspace{3mm}

Start with an AR(1)

$$
x_{t} = \phi x_{t-1} + \varepsilon_{t}
$$

\vspace{3mm}

Recursively substituting

$$
\begin{aligned}
x_{t} &= \phi(\phi x_{t-2} + \varepsilon_{t-1}) + \varepsilon_{t} = \phi^{2} x_{t-2} + \phi \varepsilon_{t-1} + \varepsilon_{t} \\
x_{t} &= \phi^{k} x_{t-k} + \phi^{k-1} \varepsilon_{t-k+1} + \cdots + \phi^{2} \varepsilon_{t-2} + \phi \varepsilon_{t-1} + \varepsilon_{t} \\
\end{aligned}
$$

\vspace{3mm}

Thus an AR(1) can always be expressed as an ARMA(k,k-1).


##

Also, if $|\phi| < 1$ so that $$\lim_{k\to\infty} \phi^{k} x_{t-k} = 0$$ then

$$
x_{t} = \sum\limits_{j=0}^{\infty} \phi^{j} \varepsilon_{t-j}
$$

\vspace{5mm}

### __AR(1) to MA($\infty$) with Lag Operators__

\vspace{3mm}

Starting again with the AR(1) model:

$$
(1 - \phi L) x_{t} = \varepsilon_{t}
$$

\vspace{3mm}

The way to "invert"  the AR(1) is to write

$$
x_{t} = (1 - \phi L)^{-1} \varepsilon_{t}
$$


## 

What does $(1 - \phi L)^{-1}$ mean? We have only defined polynomials in $L$ so far.

\vspace{3mm}

We try to use the expression

$$
(1 - z)^{-1} = 1 + z + z^{2} + z^{3} + \cdots \quad \mbox{for} \quad |z| < 1
$$

__NB:__ this expression for $z$ can be proven with a Taylor expansion.

\vspace{3mm}

Using this expansion and hoping that $|\phi| < 1$ implies $|\phi L| < 1$, suggests

\vspace{3mm}

$$
x_{t} = (1 - \phi L)^{-1} \varepsilon_{t} = (1 + \phi L + \phi^{2} L^{2}  + \cdots) \varepsilon_{t} = \sum\limits_{j=0}^{\infty} \phi^{j} \varepsilon_{t-j}
$$


##

Note: we can't always perform this inversion. We require $|\phi| < 1$. Not all ARMA processes are invertible to a representation of $x_{t}$ in terms of current and past $\varepsilon_{t}$

\vspace{2mm}

### ___AR(p) to MA($\infty$)___

Getting to an MA($\infty$) from an AR(1) is almost as easy either way (recursive substitution or lag operators) but in higher order models lag operators become much easier. 

\vspace{2mm}

Let's try an AR(2):

$$
\begin{aligned}
x_{t} &= \phi_{1} x_{t-1} + \phi_{2} x_{t-2}  + \varepsilon_{t} \\
(1 - \phi_{1} L - \phi_{2} L^{2}) x_{t} &= \varepsilon_{t}
\end{aligned}
$$

##

We need to factor $(1 - \phi_{1} L - \phi_{2} L^{2})$ in order to use the $(1 - z)^{-1}$ formula. So find $\lambda_{1}$ and $\lambda_{2}$ such that

\vspace{3mm}

$$
(1 - \phi_{1} L - \phi_{2} L^{2}) = (1 - \lambda_{1} L) (1 - \lambda_{2} L)
$$

\vspace{3mm}

The solution is 

$$
\begin{aligned}
\lambda_{1} \lambda_{2}   &= -\phi_{2} \\
\lambda_{1} + \lambda_{2} &= \phi_{1} 
\end{aligned}
$$


## Some Mathematical Details

$$
\begin{aligned}
(1 - \lambda_{1} L) (1 - \lambda_{2} L) &= 1 - \lambda_{2} L - \lambda_{1} L + \lambda_{1} \lambda_{2} L \\
                                        &= 1 - (\lambda_{1} + \lambda_{2})L + \lambda_{1} \lambda_{2} L
\end{aligned}
$$


## 

Now we need to invert

$$
(1 - \lambda_{1} L) (1 - \lambda_{2} L) x_{t} = \varepsilon_{t}
$$

\vspace{3mm}

Thus

$$
\begin{aligned}
x_{t} &= (1 - \lambda_{1} L)^{-1} (1 - \lambda_{2})^{-1} x_{t} = \varepsilon_{t} \\
x_{t} &= \left[\sum\limits_{j=0}^{\infty} \lambda_{1}^{j} L^{j}\right] \left[\sum\limits_{j=0}^{\infty} \lambda_{2}^{j} L^{j} \right] \varepsilon_{t}
\end{aligned}
$$

\vspace{3mm}

Multiplying out the polynomials is tedious but straight forward

\vspace{3mm}

$$
\begin{aligned}
\left[\sum\limits_{j=0}^{\infty} \lambda_{1}^{j} L^{j}\right] \left[\sum\limits_{j=0}^{\infty} \lambda_{2}^{j} L^{j} \right] &= (1 + \lambda_{1} L + \lambda_{2} L^{2} + \cdots) (1 + \lambda_{2} L + \lambda_{2} L^{2} + \cdots) \\
&= 1 + (\lambda_{1} + \lambda_{2}) L + (\lambda_{1}^{2} + \lambda_{1} \lambda_{2} + \lambda_{2}^{2}) L^{2} + \cdots \\
&= \sum\limits_{j=0}^{\infty} \left(\sum\limits_{k=0}^{j} \lambda_{1}^{k} \lambda_{2}^{j-k}\right) L^{j}
\end{aligned}
$$


##

A nicer way to express an MA($\infty$) is to use the __partial fractions trick__. Find $a$ and $b$ such that

\vspace{3mm}

$$
\begin{aligned}
\frac{1}{(1 - \lambda_{1} L) (1 - \lambda_{2} L)} &= \frac{a}{(1 - \lambda_{1} L)} + \frac{b}{(1 - \lambda_{2} L)} \\
&= \frac{a(1 - \lambda_{2} L) + b(1 - \lambda_{1} L)}{(1 - \lambda_{1} L)(1 - \lambda_{2} L)}
\end{aligned}
$$

\vspace{3mm}

The right-hand side numerator must equal $1$, so

$$
\begin{aligned}
a + b &= 1 \\
a \lambda_{2} + b \lambda_{1} & = 0
\end{aligned}
$$

\vspace{3mm}

The solution is

$$
b = \frac{\lambda_{2}}{\lambda_{2} - \lambda_{1}}, \quad a = \frac{\lambda_{1}}{\lambda_{1} - \lambda_{2}}
$$


##

$$
\frac{1}{(1 - \lambda_{1} L) (1 - \lambda_{2} L)} = \frac{\lambda_{1}}{(\lambda_{1} - \lambda_{2})} \frac{1}{(1 - \lambda_{1} L)} + \frac{\lambda_{2}}{(\lambda_{2} - \lambda_{1})} \frac{1}{(1 - \lambda_{2} L)}
$$

\vspace{3mm}

Thus we can express $x_{t}$ as

\vspace{3mm}

$$
\begin{aligned}
x_{t} &= \frac{\lambda_{1}}{\lambda_{1} - \lambda_{2}} \sum\limits_{j=0}^{\infty} \lambda_{1}^{j} \varepsilon_{t-j} + \frac{\lambda_{2}}{\lambda_{2} - \lambda_{1}} \sum\limits_{j=0}^{\infty} \lambda_{2}^{j} \varepsilon_{t-j} \\
&= \sum\limits_{j=0}^{\infty} \left( \frac{\lambda_{1}}{\lambda_{1} - \lambda_{2}}  \lambda_{1}^{j} + \frac{\lambda_{2}}{\lambda_{2} - \lambda_{1}} \lambda_{2}^{j} \right) \varepsilon_{t-j}
\end{aligned}
$$

##

__Note:__ Again not every AR(2) can be inverted. We require that the $\lambda$'s satisfy $|\lambda| < 1$.

\vspace{3mm}

Until explicitly stated we will assume we are working with invertible ARMA models.

\vspace{3mm}

___MA(q) to AR($\infty$)___

\vspace{2mm}

This is now straight forward

$$
x_{t} = b(L) \varepsilon_{t}
$$

\vspace{3mm}

has AR($\infty$) representation

\vspace{3mm}

$$
b(L)^{-1} x_{t} = \varepsilon_{t}
$$
