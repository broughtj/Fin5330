{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Brough Lecture Notes: Beginning Time Series Topics II__\n",
    "\n",
    "<br>\n",
    "\n",
    "Finance 5330: Financial Econometrics <br>\n",
    "Tyler J. Brough <br>\n",
    "Last Updated: January 25, 2019 <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Time Series Continued___\n",
    "\n",
    "<br>\n",
    "\n",
    "Summary of allowed lag polynomial manipulations\n",
    "\n",
    "1. We can multiply them\n",
    "\n",
    "$$\n",
    "a(L)b(L) = (a_{0} + a_{1} L + \\cdots) (b_{0} + b_{1} L + \\cdots) = a_{0}b_{0} + (a_{0}b_{1} + b_{0}a_{1}) L + \\cdots\n",
    "$$\n",
    "\n",
    "2. They commute\n",
    "\n",
    "$$\n",
    "a(L)b(L) = b(L)a(L)\n",
    "$$\n",
    "\n",
    "3. We can raise them to positive integer powers\n",
    "\n",
    "$$\n",
    "a(L)^{2} = a(L)a(L)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We can invert them, by factoring them and inverting each term\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a(L)      &= (1 - \\lambda_{1} L) (1 - \\lambda_{2} L) \\quad \\cdots \\\\\n",
    "a(L)^{-1} &= (1 - \\lambda_{1} L)^{-1} (1 - \\lambda_{2} L)^{-1} \\\\\n",
    "          &= \\sum\\limits_{j=0}^{\\infty} \\lambda_{1}^{j} L^{j} \\sum\\limits_{j=0}^{\\infty} \\lambda_{2}^{j} L^{j} \\\\\n",
    "          & = c_{1} (1 - \\lambda_{1} L)^{-1} + c_{2} (1 - \\lambda_{2} L)^{-1} \\quad \\cdots \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "We'll look at roots greater than and/or equal to one, fractional powers, and non-polynomial functions of lag operators later. \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Multivariate ARMA Models___\n",
    "\n",
    "<br>\n",
    "\n",
    "The multivariate case is similar, reinterpreting our variables as vectors and matrices:\n",
    "\n",
    "$$\n",
    "x_{t} = \\begin{bmatrix} y_{t} \\\\ z_{t} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The building block is the multivariate white noise process, $\\varepsilon_{t} \\sim iid N(0, \\Sigma)$, which we write as follows\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\varepsilon_{t} = \\begin{bmatrix} \\delta_{t} \\\\ \\nu_{t} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "with \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E(\\varepsilon_{t}) &= 0 \\\\\n",
    "E(\\varepsilon_{t} \\varepsilon_{t}^{\\prime}) &= \\Sigma = \\begin{bmatrix} \\sigma_{\\delta}^{2} & \\sigma_{\\delta \\nu}^{2} \\\\ \\sigma_{\\nu \\delta}^{2} & \\sigma_{\\nu}^{2} \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "E(\\varepsilon_{t} \\varepsilon_{t-j}^{\\prime}) = 0\n",
    "$$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AR(1) is $x_{t} = \\phi x_{t-1} + \\varepsilon_{t}$, or \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} y_{t} \\\\ z_{t} \\end{bmatrix} = \\begin{bmatrix} \\phi_{yy} & \\phi_{yz} \\\\ \\phi_{zy} & \\phi_{zz} \\end{bmatrix} \\begin{bmatrix} y_{t-1} \\\\ z_{t-1} \\end{bmatrix} + \\begin{bmatrix} \\delta_{t} \\\\ \\nu_{t} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Or\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{t} &= \\phi_{yy} y_{t-1} + \\phi_{yz} z_{t-1} + \\delta_{t} \\\\\n",
    "z_{t} &= \\phi_{zy} y_{t-1} + \\phi_{zz} z_{t-1} + \\nu_{t}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "__NB:__ this is a Vector autoregressive model of order 1, or a VAR(1) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "__NB:__ both lagged $y$ and lagged $z$ appear in each equation. \n",
    "\n",
    "<br>\n",
    "\n",
    "Thus, the VAR(1) captures cross-variable dynamics. \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "__Ex:__ It could capture the fact that if volume is higher in one trading period, volatility tends to be higher the following trading period; as well as the fact theat if volatility is high one period volume tends to be high the next period. \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write the VAR(1) in lag operator notation:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "(I - \\Phi L) x_{t} = \\varepsilon_{t}\n",
    "$$\n",
    "\n",
    "or \n",
    "\n",
    "$$\n",
    "A(L) x_{t} = B(L) \\varepsilon_{t}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "where: \n",
    "\n",
    "* $A(L) = I - \\Phi_{1} L - \\Phi_{2} L^{2} - \\cdots$\n",
    "\n",
    "* $B(L) = I + \\Theta_{1} L + \\Theta_{2} L^{2} + \\cdots$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "__NB:__ $\\Phi_{j} = \\begin{bmatrix} \\phi_{j,yy} & \\phi_{j, yz} \\\\ \\phi_{j,zy} & \\phi_{j,zz} \\end{bmatrix}$ and similarly for $\\Theta_{j}$.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can invert multivariate ARMA models. \n",
    "\n",
    "<br>\n",
    "\n",
    "For example, the MA($\\infty$) representation can be obtained from the VAR(1) as\n",
    "\n",
    "$$\n",
    "(I - \\Phi L) x_{t} = \\varepsilon_{t}  \\quad \\Leftrightarrow \\quad (I - \\Phi L)^{-1} \\varepsilon_{t} = \\sum\\limits_{j=0}^{\\infty} \\Phi^{j} \\varepsilon_{t-j}\n",
    "$$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Autocorrelation and Autocovariance Functions___\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "__Autocovariance__ of a series $x_{t}$ is\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\gamma_{j} = Cov(x_{t}, x_{t-j})\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Hence, \n",
    "\n",
    "$$\n",
    "\\gamma_{j} = E(x_{t} x_{t-j})\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "__NB:__ $\\gamma_{0} = Var(x_{t})$\n",
    "\n",
    "<br>\n",
    "\n",
    "__NB:__ Recall $Cov(x_{t}, x_{t-j}) = E[(x_{t} - E(x_{t}))(x_{t-j} - E(x_{t-j}))]$ but $E(x_{t}) = 0$ for our purposes.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Autocorrelation__ is:\n",
    "\n",
    "$$\n",
    "\\rho_{j} = \\frac{\\gamma_{j}}{Var(x_{t})} = \\frac{\\gamma_{j}}{\\gamma_{0}}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "___Autocovariance and Autocorrelation of ARMA Processes___\n",
    "\n",
    "<br>\n",
    "\n",
    "White noise: since we assume $\\varepsilon_{t} \\sim N(0,\\sigma_{\\varepsilon}^{2})$, it's clear  that\n",
    "\n",
    "$$\n",
    "\\gamma_{0} = \\sigma_{\\varepsilon_{t}}^{2}, \\quad \\gamma_{j} = 0 \\quad \\mbox{for all} \\quad j \\ne 0\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\rho_{0} = 1, \\quad \\rho_{j} = 0 \\quad \\mbox{for all} \\quad j \\ne 0\n",
    "$$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__MA(1)__\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "x_{t} = \\varepsilon_{t} + \\theta \\varepsilon_{t-1}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Autocovariance:\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\gamma_{0} &= Var(x_{t}) = Var(\\varepsilon_{t} + \\theta \\varepsilon_{t-1}) \\\\\n",
    "           &= \\sigma_{\\varepsilon}^{2} + \\theta^{2} \\sigma_{\\varepsilon}^{2} \\\\ \n",
    "           &= (1 + \\theta^{2}) \\sigma_{\\varepsilon}^{2} \\\\\n",
    "\\gamma_{1} &= E(x_{t} x_{t-1}) = E[(\\varepsilon_{t} + \\theta \\varepsilon_{t-1}) (\\varepsilon_{t-1} + \\theta \\varepsilon_{t-2})] \\\\\n",
    "           &= E[\\theta \\varepsilon_{t-1}^{2}] \\\\\n",
    "           &= \\theta \\sigma_{\\varepsilon}^{2} \\\\\n",
    "\\gamma_{2} &= E(x_{t} x_{t-2}) = E[(\\varepsilon_{t} + \\theta \\varepsilon_{t-1}) (\\varepsilon_{t-1} + \\theta \\varepsilon_{t-3})] = 0 \\\\\n",
    "\\gamma_{3} &= 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Autocorrelation\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\rho_{1} &= \\frac{\\theta}{1 + \\theta^{2}} \\\\\n",
    "\\rho_{2} &= 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__MA(2)__\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "x_{t} = \\varepsilon_{t} + \\theta_{1} \\varepsilon_{t-1} + \\theta_{2} \\varepsilon_{t-2}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Autocovariance:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\gamma_{0} &= E[(\\varepsilon_{t} + \\theta_{1} \\varepsilon_{t-1} + \\theta_{2} \\varepsilon_{t-2})(\\varepsilon_{t} + \\theta_{1} \\varepsilon_{t-1} + \\theta_{2} \\varepsilon_{t-2})] \\\\\n",
    "           &= (1 + \\theta_{1}^{2} + \\theta_{2}^{2}) \\sigma_{\\varepsilon}^{2} \\\\\n",
    "\\gamma_{1} &= E[(\\varepsilon_{t} + \\theta_{1} \\varepsilon_{t-1} + \\theta_{2} \\varepsilon_{t-2})(\\varepsilon_{t-1} + \\theta_{1} \\varepsilon_{t-2} + \\theta_{2} \\varepsilon_{t-3})] \\\\\n",
    "           &= (\\theta_{1} + \\theta_{1}\\theta_{2}) \\sigma_{\\varepsilon}^{2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$\\gamma_{3}, \\gamma_{4}, \\ldots = 0$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autocorrelation: \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\rho_{0} &= 1 \\\\\n",
    "\\rho_{1} &= \\frac{\\theta_{1} + \\theta_{1} \\theta_{2}}{(1 + \\theta_{1}^{2} + \\theta_{2}^{2})} \\\\\n",
    "\\rho_{2} &= \\frac{\\theta_{2}}{(1 + \\theta_{1}^{2} + \\theta_{2}^{2})}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$\\rho_{3}, \\rho_{4}, \\ldots = 0$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__MA(q), MA($\\infty$)__\n",
    "\n",
    "<br>\n",
    "\n",
    "By now the pattern is clear: MA(q) processes have q autocorrelations different from zero. Also, if\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "x_{t} = \\theta(L) \\varepsilon_{t} = \\sum\\limits_{j=0}^{\\infty} (\\theta_{j} L^{j}) \\varepsilon_{t}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "then \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\gamma_{0} &= Var(x_{t}) = \\left( \\sum\\limits_{j=0}^{\\infty} \\theta_{j}^{2} \\right) \\sigma_{\\varepsilon}^{2} \\\\\n",
    "\\gamma_{k} &= \\sum\\limits_{j=0}^{\\infty} \\theta_{j} \\theta_{j+k} \\sigma_{\\varepsilon}^{2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "__NB:__ $\\theta_{0} = 1$\n",
    "\n",
    "<br>\n",
    "\n",
    "__NB:__ The lesson is that calculation of 2nd moments for MA processes is easy. Because covariance terms\n",
    "$E(\\varepsilon_{j} \\varepsilon_{k})$ drop out!\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__AR(1)__\n",
    "\n",
    "<br>\n",
    "\n",
    "Two ways to proceed: \n",
    "\n",
    "1. Invert the MA($\\infty$) and use the above\n",
    "\n",
    "$$\n",
    "(1 - \\phi L) x_{t} = \\varepsilon_{t} \\Rightarrow x_{t} = (1 - \\phi L)^{-1} \\varepsilon_{t} = \\sum\\limits_{j=0}^{\\infty} \\phi^{j} \\varepsilon_{t-j} \n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\gamma_{0} &= \\left(\\sum\\limits_{j=0}^{\\infty} \\phi^{2j}\\right) \\sigma_{\\varepsilon}^{2} = \\frac{1}{1 - \\phi^{2}} \\sigma_{\\varepsilon}^{2}; \\quad \\rho_{0} = 1 \\\\\n",
    "\\gamma_{1} &= \\left(\\sum\\limits_{j=0}^{\\infty} \\phi^{j} \\phi^{j+1} \\right) \\sigma_{\\varepsilon}^{2} = \\phi \\left( \\sum\\limits_{j=0}^{\\infty} \\phi^{2j} \\right) \\sigma_{\\varepsilon}^{2}  = \\frac{\\phi}{1 - \\phi^{2}}\\sigma_{\\varepsilon}^{2}; \\quad \\rho_{1} = \\phi \\\\\n",
    "\\mbox{and continuing} \\\\\n",
    "\\gamma_{k} &= \\frac{\\phi^{k}}{1 - \\phi^{2}} \\sigma_{\\varepsilon}^{2}; \\quad \\rho_{k} = \\phi^{k}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Another way useful in it's own right\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\gamma_{1} &= E(x_{t} x_{t-1}) = E[(\\phi x_{t-1} + \\varepsilon_{t}) (x_{t-1})] = \\phi \\sigma_{x}^{2}; \\quad \\rho = \\phi \\\\\n",
    "\\gamma_{2} &= E(x_{t} x_{t-2}) = E[(\\phi^{2} x_{t-2} + \\phi \\varepsilon_{t-1} + \\varepsilon_{t}) (x_{t-2})] = \\phi^{k} \\sigma_{x}^{2}; \\quad \\rho_{k} = \\phi^{k} \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "__AR(p) Yule-Walker Equations__\n",
    "\n",
    "<br>\n",
    "\n",
    "This latter method is the easiest way to proceed for AR(p)'s. \n",
    "\n",
    "<br>\n",
    "\n",
    "Let's look at an AR(3), then you can generalize.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying both sides by $x_{t}, x_{t-1}, \\cdots$ taking expectations, then dividing by $\\gamma_{0}$ we obtain\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "1 &= \\phi \\rho_{1} + \\phi_{2} \\rho_{2} + \\phi_{3} \\rho_{3} + \\frac{\\sigma_{\\varepsilon}^{2}}{\\gamma_{0}} \\\\\n",
    "\\rho_{1} &= \\phi_{1} + \\phi_{2} \\rho_{1} + \\phi_{3} \\rho_{2}             \\\\\n",
    "\\rho_{2} &= \\phi_{1}\\rho_{1} + \\phi_{2} + \\phi_{3} \\rho_{1}              \\\\\n",
    "\\rho_{3} &= \\phi_{1}\\rho_{2} + \\phi_{2}\\rho_{1} + \\phi_{3}               \\\\\n",
    "\\rho_{k} &= \\phi_{1}\\rho_{k-1} + \\phi_{2}\\rho_{k-2} + \\phi_{3}\\rho_{k-3} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2nd, 3rd, and 4th equations can be solved for $\\rho_{1}$, $\\rho_{2}$, $\\rho_{3}$.\n",
    "\n",
    "<br>\n",
    "\n",
    "Then each remaining equation gives $\\rho_{k}$ in terms of $\\rho_{k-1}$ and $\\rho_{k-2}$, so we can solve for all of the $\\rho$'s.\n",
    "\n",
    "<br>\n",
    "\n",
    "__NB:__ The $\\rho$'s follow the same difference equation as the original $x$'s\n",
    "\n",
    "<br>\n",
    "\n",
    "The first equation can be solved for the variance\n",
    "\n",
    "$$\n",
    "\\sigma_{x}^{2} = \\gamma_{0} = \\frac{\\sigma_{\\varepsilon}^{2}}{1 - [\\phi_{1} \\rho_{1} + \\phi_{2} \\rho_{2} + \\phi_{3} \\rho_{3}]}\n",
    "$$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Stationarity___\n",
    "\n",
    "<br>\n",
    "\n",
    "In calculating the moments of ARMA processes, the moments did not depend on calendar time\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E(x_{t})         &= E(x_{s}) \\quad \\quad \\mbox{for all $t$ and $s$}         \\\\\n",
    "E(x_{t} x_{t-j}) &= E(x_{s} x_{s-j}) \\quad \\mbox{for all $t$ and $s$} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "These properties are true for the invertible ARMA models, but they reflect a deeper property.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A process $\\{x_{t}\\}$ is __strongly stationary__ or __strictly stationary__ if the joint probability distribution function of $\\{x_{t-s}, \\cdots, x_{t}, \\cdots, x_{t+s}\\}$ is independent of $t$ for all $s$.\n",
    "\n",
    "<br>\n",
    "\n",
    "A process $\\{x_{t}\\}$ is __weakly stationary__ or __covariance stationary__ if $E(x_{t})$, $E(x_{t}^{2})$ are finite and $E(x_{t} x_{t-j})$ depends only on $j$ and not on $t$.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NB:__\n",
    "\n",
    "<br>\n",
    "\n",
    "1. Strong stationary does not imply weak stationarity. $E(x_{t}^{2})$ must be finite.\n",
    "    - Ex: on iid Cauchy process is strongly, but not covariance stationary. \n",
    "    \n",
    "2. Strong stationarity plus $E(x_{t}), E(x_{t}^{2}) < \\infty \\Rightarrow$ weak stationarity\n",
    "\n",
    "3. Weak stationarity does not $\\Rightarrow$ strong stationarity. If the process is not normal, other\n",
    "   moments $\\left(E(x_{t} x_{t-j} x_{t-k})\\right)$ _might_ depend on $t$, so the process might not be \n",
    "   strongly stationary.\n",
    "   \n",
    "4. Weak stationarity plus normality $\\Rightarrow$ strong stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
